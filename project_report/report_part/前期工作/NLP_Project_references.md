**GPT-2 中文文本生成**

**Re-Subject：**

1. [GPT-2生成式多轮对话入门-----深入理解“用于中文闲聊的GPT2模型”项目_三重极简的博客-CSDN博客_gpt2](https://blog.csdn.net/g534441921/article/details/104312983)【Blog】 

2. https://github.com/thu-coai/CDial-GPT【[论文](GPT.pdf)】

3. [03](https://github.com/Morizeyao/GPT2-Chinese)【信度高、体量大 / 相关面广，下辖可选项多】

4. [hughqiu/GPT2-Chinese: Chinese version of GPT2 training code, using BERT or BPE tokenizer. (github.com)](https://github.com/hughqiu/GPT2-Chinese) 【中意】

   > **使用指南见：**[GPT-2中文文本训练及生成_是木子啦~的博客-CSDN博客_gpt2中文生成](https://blog.csdn.net/qq_44543774/article/details/116379722)

   

**数据集来源：**

1. [大规模中文自然语言处理语料 Large Scale Chinese Corpus for NLP (github.com)](https://github.com/brightmart/nlp_chinese_corpus)
2. [THUCTC: 一个高效的中文文本分类工具 (thunlp.org)](http://thuctc.thunlp.org/#中文文本分类数据集THUCNews)
3. 还可参见 Research.pdf 的内容